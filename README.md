# HCLTech_Hackathon_GenAI

## ğŸ—ï¸ System Design

The Mini RAG-powered GenAI Assistant is designed as a modular system with clear separation between:

- **Frontend** (Streamlit Web App)
- **Backend RAG Pipeline** (ingestion, chunking, embedding, retrieval, generation)
- **Vector Store** (ChromaDB)
- **LLM & Embedding Services** (Gemini)

---

### ğŸ”¹ High-Level Design

At a high level, the system works in two main phases:

1. **Indexing Phase (Offline / Background)**
   - Users upload documents (PDF, JSON, TXT, URLs, internal docs)
   - Documents are preprocessed, chunked, embedded using Gemini
   - Embeddings + metadata are stored in ChromaDB

2. **Query Phase (Online / User Interaction)**
   - User asks a question via the Streamlit UI
   - Query is embedded using Gemini
   - ChromaDB performs semantic search over stored embeddings
   - Top-k relevant chunks are retrieved as context
   - Context + query are passed to Gemini LLM to generate the final answer

---

### ğŸ”¹ Component Design

#### 1. Streamlit Frontend

- Upload multiple documents
- Show upload status and document list
- Provide a text input box for user queries
- Display:
  - Final LLM answer
  - (Optional) Retrieved context snippets
- Trigger backend functions (ingestion / query) via Python callbacks

#### 2. Document Ingestion & Preprocessing Module

**Responsibilities:**
- Accept different file types:
  - `.pdf`, `.json`, `.txt`, URLs, internal dummy docs
- Extract raw text:
  - PDF â†’ text (e.g., using PyPDF)
  - JSON â†’ relevant fields to text
  - TXT â†’ direct reading
  - URL â†’ HTTP GET + HTML parsing (e.g., BeautifulSoup)
- Clean text (remove extra spaces, HTML tags, etc.)
- Split text into **chunks**:
  - Fixed-size windows (e.g., N tokens/characters)
  - Optional overlap for better context retention
- Attach metadata:
  - `source`, `file_name`, `page_number`, `chunk_id`, etc.

#### 3. Embedding & Storage Module

**Responsibilities:**
- Call **Gemini Embeddings API** to convert text chunks into dense vectors
- Create / load a **ChromaDB collection**
- Store:
  - `embedding` (vector)
  - `text` (chunk content)
  - `metadata` (source, page, etc.)

This module abstracts the vector store so that:
- ChromaDB can be swapped out with another DB in the future with minimal changes.

#### 4. Query & Retrieval Module

**Responsibilities:**
- Take user query from the frontend
- Generate **query embedding** using Gemini
- Perform **semantic search** in ChromaDB:
  - `top_k` similar chunks are retrieved based on vector similarity
- Optionally:
  - Rank / filter results further (e.g., based on score threshold)
  - Deduplicate overlapping chunks

The output is a set of **context chunks** that are most relevant to the query.

#### 5. RAG (LLM Orchestration) Module

**Responsibilities:**
- Build a **prompt** for Gemini LLM that contains:
  - User query
  - Retrieved context chunks
  - System instructions (e.g., "Answer only using the given context")
- Send prompt to **Gemini LLM**
- Receive and return the final **answer** text
- Optionally:
  - Return sources / context chunks along with the answer

---

### ğŸ”¹ Data Flow

```text
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Streamlit UI      â”‚
          â”‚(upload + ask query) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Ingestion Module     â”‚
          â”‚ (parse, clean,       â”‚
          â”‚  chunk documents)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ chunks
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Embedding Module     â”‚
          â”‚  (Gemini Embeddings) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ embeddings + metadata
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   ChromaDB Store     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         User Query â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Query Module        â”‚
          â”‚ (embed + semantic    â”‚
          â”‚  search in ChromaDB) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ top-k context
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   RAG / LLM Module   â”‚
          â”‚ (Gemini LLM answer   â”‚
          â”‚   using context)     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Streamlit UI       â”‚
          â”‚  (show response)     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
### Folder Structure
mini-rag-assistant/
â”‚
â”œâ”€â”€ app.py                     # Streamlit entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example               # GEMINI_API_KEY placeholder
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py            # config (chunk size, top_k, etc.)
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ ingestion.py           # document loading, cleaning, chunking
â”‚   â”œâ”€â”€ embeddings.py          # Gemini embedding helpers
â”‚   â”œâ”€â”€ vector_store.py        # ChromaDB wrapper
â”‚   â”œâ”€â”€ rag_pipeline.py        # end-to-end RAG orchestration
â”‚   â””â”€â”€ utils.py               # shared helpers (logging, text utils)
â”‚
â””â”€â”€ data/
    â”œâ”€â”€ docs/                  # sample documents
    â””â”€â”€ chroma_db/             # local ChromaDB persistence (if used)
